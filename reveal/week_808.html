<!doctype html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <title>FNGS 8/8</title>

    <link rel="stylesheet" href="css/reveal.css">
    <link rel="stylesheet" href="css/theme/black.css">

    <!-- Theme used for syntax highlighting of code -->
    <link rel="stylesheet" href="lib/css/zenburn.css">

    <!-- Printing and PDF exports -->
    <script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>

    <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>


</head>
<body>
<div class="reveal">
    <div class="slides">
        <section data-background="#000066">
            <h1>Design Team 0 Slides</h1>
            <h2>Eric Bridgeford, Albert Lee, Eric Walker</h2>
        </section>

        <section data-background="#000066">
            <h1>FNGS pipeline leg overview</h1>
            <h2>Eric Bridgeford</h2>
        </section>

        <section data-background="#000066">
            <h1>Week's Accomplishments</h1>
            <ul>
                <li>Implemented Linear and Quadratic Drift removal (using CPAC methods)</li>
                <li>Finished a pipeline and analyzed for FNIRT brains (compared ts to kf-ts)</li>
                <li>Decided AGAINST smoothing</li>
        </section>

        <section data-background="#000066">
            <h1>Results for Pipeline</h1>
            <ul>
                <li>Note: first version of pipeline implementing FNIRT</li>
            </ul>
        </section>

        <section data-background="#000066">
            <h1>HNU1, no kalman filter</h1>
            <img src="images/week_81/HNU_1_no_kf.png" height="400">
        </section>

        <section data-background="#000066">
            <h1>HNU1, kalman filter</h1>
            <img src="images/week_81/HNU_1_kf.png" height="400">
        </section>

        <section data-background="#000066">
            <h1>NKI, no kalman filter</h1>
            <img src="images/week_81/NKI_no_kf.png" height="400">
        </section>

        <section data-background="#000066">
            <h1>NKI, kalman filter</h1>
            <img src="images/week_81/NKI_kf.png" height="400">
        </section>
        <section data-background="#000066">
        <h1>DC1, no kalman filter</h1>
        <img src="images/week_81/DC_1_no_kf.png" height="400">
    </section>

        <section data-background="#000066">
            <h1>DC1, kalman filter</h1>
            <img src="images/week_81/DC_1_kf.png" height="400">
        </section>

        <section data-background="#000066">
            <h1>BNU1, no kalman filter</h1>
            <img src="images/week_81/BNU_1_no_kf.png" height="400">
        </section>

        <section data-background="#000066">
            <h1>BNU1, kalman filter</h1>
            <img src="images/week_81/BNU_1_kf.png" height="400">
        </section>

        <section data-background="#000066">
            <h1>What this Means</h1>
            <ul>
                <li>With some randomly chosen DLM, Kalman filtering helps</li>
                <li>Pessimistic: might not actually work, since this DLM is completely arbitrary</li>
                <li>Need to find how to optimize the DLM, so that when we filter we are doing it right</li>
            </ul>
        </section>

        <section data-background="#000066">
            <h1>Quality Control for fMRI graphs</h1>
            <ul>
                <li>Goal: create a repo for fMRI graphs that demonstrates between-dataset variances</li>
                <li>Similar to neurodata/dataset-variance, but those metrics don't really apply</li>
                <li>Decide which metrics to include</li>
                <li>Want to be able to run all of our discriminability analyses, and adds another layer of benchmarking for each pipeline as we improve</li>
            </ul>
        </section>

        <section data-background="#000066">
            <h1>Things to do this week</h1>
            <ul>
                <li>Merge ndmg-master into ndmg-eric-dev</li>
                <li>Run conversions to binary files from HCP dataset</li>
                <li>Figure out top n-component extraction for our pipeline</li>
                <li>Make R style guide and distribute ASAP</li>

            </ul>
        </section>

        <section data-background="#000066">
            <h1>Autoregression on Timeseries</h1>
            <h2>Eric Walker</h2>
        </section>
        
        <section data-background="#000066">
            <h1>This week</h1>
            <ul>
                <li><a href="http://upward-spiral-science.github.io/fngs/reveal/week_81.html#/15">Last week's slides</a></li>
                <li>Tried different parameters for Kalman filter and examined effects</li>
            </ul>
        </section>
        
        <section data-background="#000066">
            <h1>Default parameters</h1>
            <img src="../ewok_weekly/AR(1)_KF_default.jpeg" height="400">
            <li>dlm(FF=1, GG=1, V=0.8, W=0.1, m0=0, C0=1e7)</li>
            <li>R^2 = -7.56</li>
        </section>
        
        <section data-background="#000066">
            <h1>MLE</h1>
            <img src="../ewok_weekly/AR(1)_KF_MLE.jpeg" height="400">
            <li>dlm(FF=0.3039547, GG=2.7184272, V=2.6155451, W=0.7186114, m0=0.4027390, C0=0.6656987)</li>
            <li>R^2 = -3.75</li>
        </section>
        
        <section data-background="#000066">
            <h1>Lowest R^2</h1>
            <img src="../ewok_weekly/AR(1)_KF_lowest.jpeg" height="400">
            <li>dlm(FF=1, GG=1, V=0.8, W=300, m0=0, C0=1e7)</li>
            <li>R^2 = -2.50</li>
        </section>

        <section data-background="#000066">
            <h1>Thoughts</h1>
            <ul>
                <li>Need better prediction model</li>
                <li>Default KF params smoothed the most visually</li>
                <li>Better R^2 value only caused by scattered points</li>
            </ul>
        </section>
        
        
        

        <!-- Divider -->

        <section data-background="#000066">
            <h1>Javascript Image Analysis</h1>
            <h2>Albert Lee</h2>
        </section>

        <section data-background="#000066">
            <h3>Accomplished this week</h3>
            <ul>
                <li>Developed code to test histogram equalization of slice by slice (useless as I'll explain later)</li>
                <li>Developed new sampling method that doesn't require the Clarity API</li>
                <li>Developed Code to sum the histograms of a 3D cube based off of Alex's code</li>
                <li>Tested different subsamples and subsampling methods</li>
                <li>Tested IBM Watson visualization</li>
                <li>Created NeuroCV github repo</li>
            </ul>
        </section>

        <section data-background="#000066">
            <h3>Iterative Histograms</h3>
            <ul>
                <li>Last week I found the histogram equalization of each slice and combined the slices</li>
                <li>This is improper technique as we are trying to find the histogram equalization of the whole volume not separate slices</li>
                <li>Iterative histograms is a different technique where you find the array dimensions and iteratively add the histograms accross the x y z axis</li>
            </ul>
        </section>

        <section data-background="#000066">
            <h3>Challenges</h3>
            <ul>
                <li>Iterative histograms are even more GPU intensive than slice by slice equalization</li>
                <li>Wasted a lot of time trying to downsample data to run the code on my 3GB laptop</li>
                <li>Clarity API and other packages difficult to install on linux</li>
                <li>A deeply subsampled (9, 6, 11) cube took several hours to process with Cortex's 18GB RAM</li>
            </ul>
        </section>

        <section data-background="#000066">
            <h3>Challenges part 2</h3>
            <ul>
                <li>After sampling an memap is created - this means nibabel is rendered useless for manipulations and I cannot convert back to .nii</li>
                <li>Due to my lack of understanding of interpolation my code might be deeply flawed</li>
                <li>Tried some manipulations with the Clarity API specifically with trying to turn the sampled array from into a nibabel friendly form however because nifti format is rather finicky when converting from array to nifti (at least with nibabel)</li>
                <li>In process of investigating alternative python based nifti image manipulation suites</li>
            </ul>
        </section>

        <section data-background="#000066">
            <h3>Sampling Methods</h3>
            <ul>
                <li>Random sampling with number generator (unreliable and ultimately uninformative)</li>
                <li>Proportional representation based on bin numbers (Difficult to implement in practice)</li>
                <li>Interpolation based estimation (Best method)</li>
                <li>Important consideration - Level 5 downsampling represents the greatest downsampled data available on the Neurodata server, additional downsamples are thus inherently unreliable</li>
            </ul>
        </section>


        <section data-background="#000066">
            <h3>Interpolation based estimation</h3>
            <ul>
                <li>Interpolation is the art of constructing new data points within a discrete range</li>
                <li>A very crude definition is "curve fitting" based on simple functions</li>
                <li>Similar to regression there is linear, exponential, etc.</li>
                <li>Experimented around however due to the lack of availability of a "check" model, I just chose to go with linear interpolation because it's the fastest</li>
            </ul>
        </section>

        <section data-background="#000066">
            <h3>Interpolation based estimations pt 2</h3>
            <ul>
                <li>What is the difference between sampling using interpolation of the zoom of an N size matrix and a simple downcale of the local mean</li>
                <li>Does higher order interpolation matter for our case</li>
            </ul>
        </section>

        <section data-background="#000066">
            <h3>Logic of iterated histograms</h3>
            <ul>
                <li>Take cube of data</li>
                <li>Find the dimensions thus creating your range parameters</li>
                <li>Sample using linear interpolation</li>
                <li>Over the sampled dimensions find the non zero points and iteratively add the histograms</li>
                <li>Construct the iterated histogram</li>
                <li>Normalize using basic histogram normalization</li>
            </ul>
        </section>

        <section data-background="#000066">
            <h3>Issues still in the progress</h3>
            <ul>
                <li>My method of converting back to a form involves creating an array of zeros based on the histogram of the data</li>
                <li>Next I use 32 as the bin size and divide each cell of the histogram by the hist_sum value</li>
                <li>I save the result as a csv file - however since I converted the 3D array into what is essentially a 2D representation of the histogram how do I convert back to either a 3D form</li>
            </ul>
        </section>

        <section data-background="#000066">
            <h3>Steps taken to solve these problems</h3>
            <ul>
                <li>Contacted the major clusters on campus - IDIES, MARCC, HPCC and asked if I could use a node of their cluster for free - No</li>
                <li>Constructed a small 3D array and tried the manipulations I was using - currently using results to help me debug</li>
                <li>Concern: Even a 2x2x2 (smallest reasonable volume) requires several hours to process</li>
                <li>Write it in C code? - probably not best use</li>
            </ul>
        </section>

        <section data-background="#000066">
            <h3>NeuroCV Repo</h3>
            <ul>
                <li>Stored Histogram equalization notes and all the various histogram equalization methods I've tested</li>
                <li>A lot of code is not included because it requires nifti raw data files which Github does not provide space for - I'll create pointers using Git lfs, but my cortex account has all the necessary files as well</li>
                <li>Jupyter notebook is partially filled out, but I need to figure out how to outsource the code - is there a nohup command for jupyter notebook? (To avoid hogging cortex for 12+ hours)</li>

            </ul>
        </section>
        
        <section data-background="#000066">
            <h3>Next week</h3>
            <ul>
                <li>Find way to prove my histogram equalization works - Problem: There are no 3D examples of histogram equalization on the web so difficult to prove</li>
                <li>Find way to improve processing time</li>
                <li>Ask Jovo for more stuff</li>
            </ul>
        </section>


    </div>
</div>

<script src="lib/js/head.min.js"></script>
<script src="js/reveal.js"></script>

<script>
			// More info https://github.com/hakimel/reveal.js#configuration
			Reveal.initialize({
				history: true,
				// More info https://github.com/hakimel/reveal.js#dependencies
				dependencies: [
					{ src: 'plugin/markdown/marked.js' },
					{ src: 'plugin/markdown/markdown.js' },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }
				]
			});
		</script>
</body>
</html>
