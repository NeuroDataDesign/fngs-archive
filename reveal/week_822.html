<!doctype html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <title>FNGS 8/15</title>

    <link rel="stylesheet" href="css/reveal.css">
    <link rel="stylesheet" href="css/theme/black.css">

    <!-- Theme used for syntax highlighting of code -->
    <link rel="stylesheet" href="lib/css/zenburn.css">

    <!-- Printing and PDF exports -->
    <script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>

    <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>


</head>
<body>
<div class="reveal">
    <div class="slides">
        <section data-background="#000066">
            <h1>Design Team 0 Slides</h1>
            <h2>Eric Bridgeford, Albert Lee, Eric Walker</h2>
        </section>

        <section data-background="#000066">
            <h1>FNGS pipeline leg overview</h1>
            <h2>Eric Bridgeford</h2>
        </section>

        <section data-background="#000066">
            <h1>Week's Accomplishments</h1>
            <ul>
                <li>ML tutorials on coursera </li>
                <li>EM for GMM</li>
                <li>Multivariate Kalman Filter Implementation</li>
                <li>Saw Black Sabbath \../ Friday</li>
            </ul>
        </section>

        <section data-background="#000066">
            <h2>New Data analyzed</h2>
            <ul>
                <li>Added high-pass filter to pipeline</li>
                <li>Goal: eliminate scanner drifting, which occurs over time in fMRI studies</li>
                <li>Theory: MR signal is a higher frequency response, so take out the low frequency portions</li>
                <li>Application: filter out all signals with a period greater than 100 seconds</li>
            </ul>
        </section>

        <section data-background="#000066">
            <h2>BNU1</h2>
            <img src="images/week_822/BNU1_no df_desikan_2mm.png" height="400">
        </section>

        <section data-background="#000066">
            <h2>BNU2</h2>
            <img src="images/week_822/BNU2_no df_desikan_2mm.png" height="400">
        </section>

        <section data-background="#000066">
            <h2>NKI</h2>
            <img src="images/week_822/NKI_no df_desikan_2mm.png" height="400">
        </section>

        <section data-background="#000066">
            <h1>Comments</h1>
            <ul>
                <li>Pipeline looks awesome; mnrs are consistently very good</li>
            </ul>
        </section>

        <section data-background="#000066">
            <h1>More Stuff I worked on</h1>
            <li><a href="pdfs/kalman_filter.pdf">Link</a> to Multivariate Kalman Filter Rmd</li>
            <li><a href="pdfs/gmm.pdf">Link</a> to Multivariate GMM Overview</li>
        </section>

        <section data-background="#000066">
            <h1>Things to do this week</h1>
            <ul>
                <li>finish fMRI variance repo</li>
                <li>use EM for estimating covariance matrices of sources for conditions given our input vec</li>
                <li>studying for GREs</li>
                <li>Enjoy beach</li>
            </ul>
        </section>

	<!-- Divider -->

        <section data-background="#000066">
            <h1>Univariate Kalman Filter and Smoother</h1>
            <h2>Eric Walker</h2>
        </section>
        
        <section data-background="#000066">
            <h1>This week</h1>
            <li><a href="../ewok_weekly/ewok_week_815.html">Link</a> to R Markdown</li>
        </section>

        <!-- Divider -->

        <section data-background="#000066">
            <h1>Javascript Image Analysis</h1>
            <h2>Albert Lee</h2>
        </section>

        <section data-background="#000066">
            <h3>Accomplished this week</h3>
            <ul>
                <li>Developed code to test histogram equalization of slice by slice (useless as I'll explain later)</li>
                <li>Developed new sampling method that doesn't require the Clarity API</li>
                <li>Developed Code to sum the histograms of a 3D cube based off of Alex's code</li>
                <li>Tested different subsamples and subsampling methods</li>
                <li>Tested IBM Watson visualization</li>
                <li>Created NeuroCV github repo</li>
            </ul>
        </section>

        <section data-background="#000066">
            <h3>Iterative Histograms</h3>
            <ul>
                <li>Last week I found the histogram equalization of each slice and combined the slices</li>
                <li>This is improper technique as we are trying to find the histogram equalization of the whole volume not separate slices</li>
                <li>Iterative histograms is a different technique where you find the array dimensions and iteratively add the histograms accross the x y z axis</li>
            </ul>
        </section>

        <section data-background="#000066">
            <h3>Challenges</h3>
            <ul>
                <li>Iterative histograms are even more GPU intensive than slice by slice equalization</li>
                <li>Wasted a lot of time trying to downsample data to run the code on my 3GB laptop</li>
                <li>Clarity API and other packages difficult to install on linux</li>
                <li>A deeply subsampled (9, 6, 11) cube took several hours to process with Cortex's 18GB RAM</li>
            </ul>
        </section>

        <section data-background="#000066">
            <h3>Challenges part 2</h3>
            <ul>
                <li>After sampling an memap is created - this means nibabel is rendered useless for manipulations and I cannot convert back to .nii</li>
                <li>Due to my lack of understanding of interpolation my code might be deeply flawed</li>
                <li>Tried some manipulations with the Clarity API specifically with trying to turn the sampled array from into a nibabel friendly form however because nifti format is rather finicky when converting from array to nifti (at least with nibabel)</li>
                <li>In process of investigating alternative python based nifti image manipulation suites</li>
            </ul>
        </section>

        <section data-background="#000066">
            <h3>Sampling Methods</h3>
            <ul>
                <li>Random sampling with number generator (unreliable and ultimately uninformative)</li>
                <li>Proportional representation based on bin numbers (Difficult to implement in practice)</li>
                <li>Interpolation based estimation (Best method)</li>
                <li>Important consideration - Level 5 downsampling represents the greatest downsampled data available on the Neurodata server, additional downsamples are thus inherently unreliable</li>
            </ul>
        </section>


        <section data-background="#000066">
            <h3>Interpolation based estimation</h3>
            <ul>
                <li>Interpolation is the art of constructing new data points within a discrete range</li>
                <li>A very crude definition is "curve fitting" based on simple functions</li>
                <li>Similar to regression there is linear, exponential, etc.</li>
                <li>Experimented around however due to the lack of availability of a "check" model, I just chose to go with linear interpolation because it's the fastest</li>
            </ul>
        </section>

        <section data-background="#000066">
            <h3>Interpolation based estimations pt 2</h3>
            <ul>
                <li>What is the difference between sampling using interpolation of the zoom of an N size matrix and a simple downcale of the local mean</li>
                <li>Does higher order interpolation matter for our case</li>
            </ul>
        </section>

        <section data-background="#000066">
            <h3>Logic of iterated histograms</h3>
            <ul>
                <li>Take cube of data</li>
                <li>Find the dimensions thus creating your range parameters</li>
                <li>Sample using linear interpolation</li>
                <li>Over the sampled dimensions find the non zero points and iteratively add the histograms</li>
                <li>Construct the iterated histogram</li>
                <li>Normalize using basic histogram normalization</li>
            </ul>
        </section>

        <section data-background="#000066">
            <h3>Issues still in the progress</h3>
            <ul>
                <li>My method of converting back to a form involves creating an array of zeros based on the histogram of the data</li>
                <li>Next I use 32 as the bin size and divide each cell of the histogram by the hist_sum value</li>
                <li>I save the result as a csv file - however since I converted the 3D array into what is essentially a 2D representation of the histogram how do I convert back to either a 3D form</li>
            </ul>
        </section>

        <section data-background="#000066">
            <h3>Steps taken to solve these problems</h3>
            <ul>
                <li>Contacted the major clusters on campus - IDIES, MARCC, HPCC and asked if I could use a node of their cluster for free - No</li>
                <li>Constructed a small 3D array and tried the manipulations I was using - currently using results to help me debug</li>
                <li>Concern: Even a 2x2x2 (smallest reasonable volume) requires several hours to process</li>
                <li>Write it in C code? - probably not best use</li>
            </ul>
        </section>

        <section data-background="#000066">
            <h3>NeuroCV Repo</h3>
            <ul>
                <li>Stored Histogram equalization notes and all the various histogram equalization methods I've tested</li>
                <li>A lot of code is not included because it requires nifti raw data files which Github does not provide space for - I'll create pointers using Git lfs, but my cortex account has all the necessary files as well</li>
                <li>Jupyter notebook is partially filled out, but I need to figure out how to outsource the code - is there a nohup command for jupyter notebook? (To avoid hogging cortex for 12+ hours)</li>
            </ul>
        </section>

        <section data-background="#000066">
            <h3>NeuroCV Repo</h3>
            <ul>
                <li><a href = "https://github.com/alee156/NeuroCV">Repo</a></li>
                <li><a href = "https://github.com/alee156/NeuroCV/blob/master/arraytest.py">Code for debug</a></li>
            </ul>
        </section>
        
        <section data-background="#000066">
            <h3>Next week</h3>
            <ul>
                <li>Find way to prove my histogram equalization works - Problem: There are no 3D examples of histogram equalization on the web so difficult to prove</li>
                <li>Find way to improve processing time</li>
                <li>Ask Jovo for more stuff</li>
            </ul>
        </section>


    </div>
</div>

<script src="lib/js/head.min.js"></script>
<script src="js/reveal.js"></script>

<script>
			// More info https://github.com/hakimel/reveal.js#configuration
			Reveal.initialize({
				history: true,
				// More info https://github.com/hakimel/reveal.js#dependencies
				math: {
                    mathjax: 'https://cdn.mathjax.org/mathjax/latest/MathJax.js',
                    config: 'TeX-AMS_HTML-full'  // See http://docs.mathjax.org/en/latest/config-files.html
                },
				dependencies: [
					{ src: 'plugin/markdown/marked.js' },
					{ src: 'plugin/math/math.js', async: true },
					{ src: 'plugin/markdown/markdown.js' },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }
				]
			});
		</script>
</body>
</html>
