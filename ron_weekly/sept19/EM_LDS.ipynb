{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expectation maximization for Linear Dynamical System Identification\n",
    "\n",
    "In this notebook, we will present a high level overall summary of the paper [An M-Estimator for Reduced-Rank High-Dimensional Linear Dynamical System Identification](http://arxiv.org/pdf/1509.03927v1.pdf), as well as a univariate implementation of the algorithm in Python.\n",
    "\n",
    "## High level summary of solution\n",
    " - Generalization of the classical Kalman Filter-Smoother Expectation-Maximization algorithm\n",
    " - Want to fit statistical model to time series data\n",
    "\n",
    "## Motivation\n",
    " - abundance of high dimensional time series data, we want to fit statistical models to them\n",
    " - Existing methods cannot cope with high dimensional nature of these problems because of computational and statistical reasons\n",
    " - Parameters for Kalman filters are often unknown - opportunity to use unsupervised learning\n",
    " - 3 main issues: dense high dimensional matrices, estimators are numerically unstable, computation time\n",
    " \n",
    "## The model\n",
    "\n",
    "Observed variables: $Y=(y_1,\\dots,y_T)$ \n",
    "\n",
    "Latent variables: $X=(x_1,\\dots,x_T)$\n",
    "\n",
    "$$P(Y|X) = \\prod_{t=1}^T P(y_t | y_{0:t-1}, x_{0:t})$$\n",
    "\n",
    "$$P(X) = P(x_0) \\prod_{t=1}^T P(x_t | x_{0:t-1})$$\n",
    "\n",
    "Time invariant state space model makes the following simplifications:\n",
    "\n",
    "$$P(y_t | y_{0:t-1}, x_{0:t})  \\approx P(y_t | x_t)$$\n",
    "\n",
    "$$P(x_t | x_{0:t-1}) \\approx P(x_t | x_{t-1})$$\n",
    "\n",
    "Linear dynamical systems (resembles Kalman filters):\n",
    "\n",
    "$$ x_{t+1}= Ax_t+Y'_t  \\quad Y'_t\\sim N(\\mathbf{0},Q),\\quad x_0 \\sim N(\\mathbf{\\pi}_0,V_0)$$\n",
    "\n",
    "$$y_t=Cx_t+\\mathbf{v}_t,\\qquad \\mathbf{v}_t\\sim N(\\mathbf{0},R)$$\n",
    "\n",
    "**Notation:**\n",
    "\n",
    "$A \\in \\mathbb{R^{d\\times d}}$ is state transformation matrix\n",
    "\n",
    "$C \\in \\mathbb{R^{p\\times d}}$ is generative matrix\n",
    "\n",
    "$x_t \\in \\mathbb{R^{d}}$\n",
    "\n",
    "$y_t \\in \\mathbb{R^{p}}$\n",
    "\n",
    "$R \\in \\mathbb{R^{p \\times p}}$ is output noise covariance matrix\n",
    "\n",
    "$Q \\in \\mathbb{R^{d \\times d}}$ is state noise covariance matrix\n",
    "\n",
    "$\\pi_0 \\in \\mathbb{R^{d}}$ is initial state mean\n",
    "\n",
    "$V_0 \\in \\mathbb{R^{d \\times d}}$ is initial state covariance\n",
    "\n",
    "The model has the following constaints to make the system identifiable and the model useful:\n",
    "\n",
    "1. $Q = I_{d \\times d}$\n",
    "2. ordering of columns of $C$ is based on their norms\n",
    "3. $V_0  = 0_{d \\times d}$\n",
    "4. $R$ is diagonal matrix\n",
    "5. $A$ is sparse\n",
    "6. $C$ has smooth columns (what does this acutally mean intuitively aside from the l_2 norm?)\n",
    "\n",
    "These constraints simplify the model even further, as:\n",
    "\n",
    "$$ x_{t+1}= Ax_t+Y'_t  \\quad Y'_t\\sim N(\\mathbf{0},I),\\quad x_0 = \\mathbf{\\pi}_0$$\n",
    "\n",
    "$$y_t=Cx_t+\\mathbf{v}_t,\\qquad \\mathbf{v}_t\\sim N(\\mathbf{0},R)$$\n",
    "\n",
    "### The main optimization problem\n",
    "\n",
    "Let $\\theta =\\{A,C,R,\\mathbf{\\pi}_0\\}$ represent all unknown parameters, and $P(X,Y)$ be the likelihood for a generic LDS model. We arrive at the optimization problem:\n",
    "\n",
    "$$\\hat{\\theta}= argmin_{\\substack{\\theta}}\\left\\{-\\log P_\\theta(X,Y)+\\lambda_1\\|A\\|_1+\\lambda_2\\|C\\|_2^2\\right\\}$$\n",
    "\n",
    "where the lambdas are tuning parameters. This can be equivalently stated as:\n",
    "\n",
    "$$\\text{minimize} \\left\\{-\\log P_\\theta(X,Y)\\right\\}$$\n",
    "\n",
    "$$ \\alpha\\|A\\|_1+ (1-\\alpha)\\|C\\|_2^2 \\leq t \\text{ for some }t; $$\n",
    "$$ A\\in \\mathcal{A}_{d\\times d},\\ C \\in \\mathcal{C}_{p \\times d}, R \\in \\mathcal{R}_{p\\times p}, \\pi_0 \\in \\mathcal{\\pi}_{d\\times 1}.$$\n",
    "\n",
    "## Paramter Estimation\n",
    "\n",
    "Main idea: we can use the EM algorithm to maximize likelihood function of observed data.\n",
    "\n",
    "The likelihood in the model is:\n",
    "$$ P(X,Y)=\\prod\\limits_{t=1}^{T}P(x_t|x_{t-1})\\prod\\limits_{t=1}^{T} P(y_t|x_t)\\mathbb{1}_{\\mathbf{\\pi}_0}(x_0) $$\n",
    "\n",
    "where $\\mathbb{1}_{\\mathbf{\\pi}_0}(x_0)$ is the indicator function and conditional likelihoods are:\n",
    "\n",
    "$$P(y_t|x_t)= (2\\pi)^{-\\frac{p}{2}}|R|^{-\\frac{1}{2}}\\  \\text{exp}\\left\\{-\\frac{1}{2}[y_t-Cx_t]^{T}R^{-1}[y_t-Cx_t]\\right\\}$$\n",
    "\n",
    "$$P(x_t|x_{t-1}) =\\text{exp}\\left\\{-\\frac{1}{2}[\\mathbf{x_t}-A\\mathbf{x_{t-1}}]^{T}Q^{-1}[\\mathbf{x_t}-A\\mathbf{x_{t-1}}]\\right\\}(2\\pi)^{-d/2}|Q|^{-1/2} =(2\\pi)^{-\\frac{d}{2}}\\  \\text{exp}\\left\\{-\\frac{1}{2}[x_t-Ax_{t-1}]^{T}[x_t-Ax_{t-1}]\\right\\}$$\n",
    "\n",
    "which allows us to further the parameter estimation simplify to:\n",
    "\n",
    "$$\\hat{\\theta}= argmin_{\\substack{\\theta}}\\biggl\\{\\sum\\limits_{t=1}^{T}\\big(\\frac{1}{2}[y_t-Cx_t]^{T}R^{-1}[y_t-Cx_t]\\big)-\\frac{T}{2}\\text{log}|R|\\\\\n",
    "+\\sum\\limits_{t=1}^{T}\\big(\\frac{1}{2}[x_t-Ax_{t-1}]^{T}[x_t-Ax_{t-1}]\\big)-\\frac{T}{2}\\text{log}|\\mathbf{I}| - \\text{log}(\\mathbb{1}_{\\mathbf{\\pi}_0}(x_0))\\\\\n",
    "+\\lambda_1\\|A\\|_1+\\lambda_2\\|C\\|_2^2\\biggr\\}.$$\n",
    "\n",
    "Let $\\mathbf{\\Phi}(\\theta,Y,X)$ denote this objective function in the curly brackets.\n",
    "\n",
    "## E and M steps\n",
    "\n",
    "### E step\n",
    "\n",
    "Calculuate expected value of log-likelihood $\\Gamma = E[\\log P(X,Y|Y)]$ (ie exactly the same as regular E step in EM)\n",
    "\n",
    "### M step\n",
    "\n",
    "$$ R^{\\text{new}} = \\text{diag} \\biggl\\{\\frac{1}{T}\\sum\\limits_{t=1}^{T}(y_ty_t^{T}-C\\hat{x}_ty_t^{T})\\biggr\\}\n",
    "$$\n",
    "\n",
    "$$\\mathbf{\\pi}_0^{\\text{new}} = \\hat{x}_0$$\n",
    "\n",
    "$$\\mathbf{c}^{\\text{new}} = (X'^{T}X' + \\lambda_2\\mathbf{I})^{-1}X'^{T}Y'$$\n",
    "\n",
    "where\n",
    "$$\n",
    " \\mathbf{c}^{\\text{new}} = (C_{11}^{\\text{new}},\\ldots,C_{1d}^{\\text{new}},C_{21}^{\\text{new}},\\ldots,C_{2d}^{\\text{new}},C_{p1}^{\\text{new}},\\ldots,C_{pd}^{\\text{new}})^{T}\n",
    "\n",
    "$$Y' = (y_{11},\\ldots,y_{T1},y_{12},\\ldots,y_{T2},\\ldots,y_{1p},\\ldots,y_{Tp})^{T}$$\n",
    "\n",
    "$$\n",
    "%\\[\n",
    "X' = \\begin{pmatrix}\n",
    "X^{T}\\\\\n",
    "\\ddots\\\\\n",
    "X^{T}\n",
    "\\end{pmatrix}_{pT\\times pd}.\n",
    "%\\]\n",
    "$$\n",
    "\n",
    "so we rearrange the vectorized version of $C^{\\text{new}}$ to get it into matrix form.\n",
    "\n",
    "$f_{\\lambda_1}(A;X,Y)$ does not have a  closed form solution, but can be solved numerically with a Fast Iterative Shrinkage-Thresholding Algorithm (FISTA).\n",
    "\n",
    "$$A^{\\text{new}} = \\text{FISTA}(\\|\\mathbf{Z}^{T}\\mathbf{a}^{\\text{old}} -\\mathbf{z}\\|_2^2,\\quad \\lambda_1)$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Initialization\n",
    "\n",
    "$$R = I$$\n",
    "\n",
    "$$\\mathbf{\\pi}_0 = \\mathbf{0}$$\n",
    "\n",
    "$C = \\mathbf{U}_{p\\times d}$ where $\\mathbf{U}_{p\\times d}$ is from the compact SVD of $Y$\n",
    "\n",
    "Columns of $X_{d \\times T}$ are used as input for a vector autoregressive (VAR) model to estimate the initial value for $A$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Implementation\n",
    "\n",
    "We implement the algorithm in the univariate setting below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
