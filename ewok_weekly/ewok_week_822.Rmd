---
title: "Multivariate Kalman Filter and Smoother"
author: "Ewok"
date: "August 22, 2016"
output:
  html_document:
    toc: true
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Equations
The following equations that govern the Kalman Filter and Smoother can be found online [here](https://cse.sc.edu/~terejanu/files/tutorialKS.pdf):

|**Model and Observation**|
|:-------------------------------|:-------------|
|Model and Observation   |$x_k = F_{k-1}x_{k-1} + B_{k-1}u_{k-1}+w_{k-1}$|
|                        |$z_k = H_kx_k+v_k$|
|**Forward Filter**| |
|Initialization | $x_{f0}^+ = \mu_0$ with error covariance $P_{f0}^+$|
|Model Forecast Step/Predictor| $x_{fk}^- = F_{k-1}x_{fk-1}^+ + B_{k-1}u_{k-1}$|
|                             | $P_{fk}^- = F_{k-1}P_{fk-1}^+F_{k-1}^T + Q_{k-1}$|
|Data Assimilation Step/Corrector|$x_{fk}^+=x_{fk}^-+K_{fk}(z_k-H_kx_{fk}^-)$|
|                                |$K_{fk} = P_{fk}^-H_k^T(H_kP_{fk}^-H_k^T+R_k)^{-1}$|
|                                |$P_{fk}^+ = (I-K_{fk}H_k)P_{fk}^-$|
|**Smoother**| |
|Initialization|$x_N^s = x_{fk}^+$|
|              |$P_N^s = P_{fk}^+$|
|Update        |$K_k^s = P_{fk}^+F_k^T(P_{fk+1}^-)^{-1}$|
|              |$P_k^s = P_{fk}^+-K_k^s(P_{fk+1}^- - P_{k+1}^s)(K_k^s)^T$|
|              |$x_k^s = x_{fk}^++K_k^s(x_{k+1}^s-x_{fk+1}^-)$|

## Kalman Filter

``` {r kalman}
# kalman_filter.R
# Created by Eric Bridgeford on 2016-08-18
# Email: ebridge2@jhu.edu
# Copyright (c) 2016. All rights reserved. 
#
# An implementation of a multivariate Kalman Filter.
#
# written by Eric Bridgeford
#
# given:
#     n is the number of variables we are tracking
#     t is the number of timesteps/observations
#     m is the number of sensors we are using
#
# NOTE: Generally, Z and X are in the forms [m, t] and [n, t]
#   respectively. Here, we use the forms [t, m] and [t, n], as these
#   are the general conventions to have observations (t) as the rows
#   and variables (n, m) as the columns.
# Inputs:
#   F[n, n]
#     - the update matrix
#     - updating the parameters for our predicted states
#   Z[t, m]
#     - the measurement vector
#     - outputs directly from the sensors
#   H[m, n]
#     - extraction matrix
#     - tells the sensor readings if we have input X
#     - ideally, Z^T = HX^T
#     - hope for this to be the identity matrix
#   R[t, m, m]
#     - variance-covariance matrix of the sensors
#   u[t, c]
#     - move matrix
#     - change to X that is known to be happening
#   B[n, 1]
#     - governs impact on each variable known change has
#   Q[k, n]
#     - the environment noise at each time step
#   p0[n, n]
#     - initial guess of the covariance matrix
#     - gives a sense of the possible noise in our estimate/process
#   x0[1, n]
#     - initial guess of the state we are in
# Outputs:
#   X[t, n]
#     - the state matrix
#   P[t, n, n]
#     - the process covariance matrices
#   
kalman_filter <- function(Fm, Z, H, R, u, B, Q, p0, x0) {
  require('MASS')
  # save key dimensions for O(1) access
  t <- dim(Z)[1]
  n <- dim(x0)[2]
  m <- dim(Z)[2]
  
  # manipulate array inputs to be the correct shape,
  # if they are not already
  # if somebody passes u as [1, c] and not [t, c]
  if (isTRUE(all.equal(dim(u)[1], 1))) {
    c = dim(u)[2]
    u = array(rep(u, t), dim=c(t, c))
  }
  # same for Q and R...
  if (isTRUE(all.equal(dim(Q)[1], 1))) {
    Q = array(rep(Q, t), dim=c(t, n))
  }
  if (isTRUE(all.equal(dim(R)[1], 1))) {
    Rold = R
    R = array(0, dim=c(t, m, m))
    for (k in 1:t) {
      R[k,,] = Rold 
    }
  }
  
  # initialize X to the dimensions of the guess and the
  # number of observations in the measurement vector
  dimX <- c(t, n)
  dimP <- c(t, n, n)
  X <- array(rep(0, prod(dimX)), dim=dimX)
  P <- array(rep(0, prod(dimP)), dim=dimP)
  X[1,] <- x0
  P[1,,] <- p0
  
  # if we don't have the same number of time steps,
  # repeat our first series for Q for each timestep
  
  for (k in 2:dim(X)[1]) {
    X[k,] <- Fm %*% X[k-1,] + B %*% u[k,]
    P[k,,] <- Fm %*% P[k-1,,] %*% t(Fm) + Q[k,]
    K <- P[k,,] %*% t(H) %*% ginv(H %*% P[k,,] %*% t(H) + R[k,,])
    X[k,] <- X[k,] + K %*% (Z[k,] - H %*% X[k,])
    P[k,,] <- (diag(n) - K %*% H) %*% P[k,,]
  }
  
  return(list(state=X, cov=P))
}
```

Take the following example:

``` {r ex1}
x = seq(0,100)
y = 0.5*x
n = 2  # number of varibales
dim = length(x)  # number of timesteps
sd = 2  # standard deviation
x_noise = x + rnorm(dim, mean=0, sd=sd)
y_noise = y + rnorm(dim, mean=0, sd=sd)
meas = array(x_noise, dim=c(dim,n))
meas[,2] <- y_noise
kf <- kalman_filter(Fm=array(c(1, 0.5, 0, 0), dim=c(n,n)),Z=meas, H=diag(n), R=array(c(sd^2, 0, 0, sd^2), dim=c(1,n,n)), u=array(c(rep(1,dim), rep(0,dim)), dim=c(dim,n)), B=array(c(1,0,0,0), dim=c(n, n)), Q=array(0.0001, dim=c(1,n)), p0=array(c(sd^2, 0, 0, sd^2), dim=c(n,n)), x0=array(c(0,0), dim=c(1,n)))
plot(x, y, main="Multivariate Kalman Filter")
lines(x_noise, y_noise, col="black")
lines(kf$state[,1], kf$state[,2], col="blue")
legend("topleft", c("Actual", "Measurement", "Kalman"), lty=c(0,1,1), pch=c(1,-1,-1), col=c("black", "black", "blue"))
rsq_kf_x <- cor(x, kf$state[,1])^2
rsq_kf_y <- cor(y, kf$state[,2])^2
rsq_kf_x
rsq_kf_y
```

Another example:

``` {r ex2}
x = seq(0,100)
y = -(0.5*(x-50))^2 + 625
n = 2  # number of varibales
dim = length(x)  # number of timesteps
sd = 5  # standard deviation
x_noise = x + rnorm(dim, mean=0, sd=sd)
y_noise = y + rnorm(dim, mean=0, sd=sd)
meas = array(x_noise, dim=c(dim,n))
meas[,2] <- y_noise
kf <- kalman_filter(Fm=array(c(1, 0, 0, 0), dim=c(n,n)),Z=meas, H=diag(n), R=array(c(sd^2, 0, 0, sd^2), dim=c(1,n,n)), u=array(c(rep(1,dim), meas[,2]), dim=c(dim,n)), B=array(c(1,0,0,1), dim=c(n, n)), Q=array(0.0001, dim=c(1,n)), p0=array(c(sd^2, 0, 0, sd^2), dim=c(n,n)), x0=array(c(0,400), dim=c(1,n)))
plot(x, y, main="Multivariate Kalman Filter")
lines(x_noise, y_noise, col="black")
lines(kf$state[,1], kf$state[,2], col="blue")
legend("topleft", c("Actual", "Measurement", "Kalman"), lty=c(0,1,1), pch=c(1,-1,-1), col=c("black", "black", "blue"))
rsq_kf_x <- cor(x, kf$state[,1])^2
rsq_kf_y <- cor(y, kf$state[,2])^2
rsq_kf_x
rsq_kf_y
```

## Kalman Smoother

``` {r smoother}
# kalman_smoother.R
# Created by Eric Bridgeford on 2016-08-18
# Email: ebridge2@jhu.edu
# Copyright (c) 2016. All rights reserved. 
#
# An implementation of a multivariate Kalman Smoother.
#
# written by Eric Bridgeford
#

kalman_smoother <- function(Fm, Z, H, R, u, B, Q, p0, x0, i=1) {
  require('MASS')
  # save key dimensions for O(1) access
  t <- dim(Z)[1]
  n <- dim(x0)[2]
  m <- dim(Z)[2]
  
  # manipulate array inputs to be the correct shape,
  # if they are not already
  # if somebody passes u as [1, c] and not [t, c]
  if (isTRUE(all.equal(dim(u)[1], 1))) {
    c = dim(u)[2]
    u = array(rep(u, t), dim=c(t, c))
  }
  # same for Q and R...
  if (isTRUE(all.equal(dim(Q)[1], 1))) {
    Q = array(rep(Q, t), dim=c(t, n))
  }
  if (isTRUE(all.equal(dim(R)[1], 1))) {
    Rold = R
    R = array(0, dim=c(t, m, m))
    for (k in 1:t) {
      R[k,,] = Rold 
    }
  }
  
  # initialize X to the dimensions of the guess and the
  # number of observations in the measurement vector
  dimX <- c(t, n)
  dimP <- c(t, n, n)
  X <- array(rep(0, prod(dimX)), dim=dimX)
  P <- array(rep(0, prod(dimP)), dim=dimP)
  
  kf <- kalman_filter(Fm, Z, H, R, u, B, Q, p0, x0)
  X[t,] <- kf$state[t,]
  P[t,,] <- kf$cov[t,,]
  
  if (isTRUE(all.equal(i, t))) {
    return(list(state=X, cov=P))
  }
  prediction <- list(state = kf$state[i,], cov = kf$cov[i,,])
  nextpred <- next_prediction(prediction$state, prediction$cov,
                              Fm=Fm, B=B, u=u[i,], Q=Q[i,])
  
  zero_ar <- array(0, dim=c(n,n))
  if (isTRUE(all.equal(nextpred$cov, zero_ar))) {
    gain <- zero_ar
  } else {
    gain <- prediction$cov %*% Fm * ginv(nextpred$cov)
  }  
  
  next_head <- kalman_smoother(Fm, Z, H, R, u, B, Q, p0, x0, i=i+1)
  X <- next_head$state
  P <- next_head$cov
  
  nextstate <- X[i+1,]
  nextcov <- P[i+1,,]
  P[i,,] <- prediction$cov - gain %*% (nextpred$cov - nextcov) %*% t(gain)
  X[i,] <- prediction$state + gain %*% (nextstate - nextpred$state)
  
  return(list(state = X, cov = P))
}

next_prediction <- function(state, cov, Fm, B, u, Q) {
  
  st <- Fm %*% state + B %*% u
  pr <- Fm %*% cov %*% t(Fm) + Q
  
  return(list(state = st, cov = pr))
}
```

``` {r reqs, echo=FALSE, message=FALSE, warning=FALSE}
require('reshape2')
require('ggplot2')
require('latex2exp')
require('MASS')
require('abind')
require('ellipse')
require('mvtnorm')
```

Here's a look at an example:

```{r ex3}
t <- seq(0, 4*pi,,100)  # simple vector
signal <- sin(t)
noise = rnorm(100, mean=0, sd=0.1)
noise[seq(1, 100, by=6)] = rnorm(17, mean=0, sd=.5)
Z <- array(signal + noise, dim=c(100, 1))
filtered <- kalman_filter(Fm=array(1, dim=c(1,1)),Z=Z, H=array(1, dim=c(1,1)),
                            R=array(.01, dim=c(1,1,1)), u=array(0, dim=c(1,1)),
                            B=array(0, dim=c(1,1)), Q=array(.001, dim=c(1,1)),
                            p0=array(.1^2, dim=c(1,1)), x0=array(0, dim=c(1,1)))
smoothed <- kalman_smoother(Fm=array(1, dim=c(1,1)),Z=Z, H=array(1, dim=c(1,1)),
                            R=array(.01, dim=c(1,1,1)), u=array(0, dim=c(1,1)),
                            B=array(0, dim=c(1,1)), Q=array(.001, dim=c(1,1)),
                            p0=array(.1^2, dim=c(1,1)), x0=array(0, dim=c(1,1)))
data <- data.frame(Timestep = t, KF=filtered$state, KS=smoothed$state, sinusoid=signal, noised=Z)
mdata <- melt(data, id=c("Timestep"))
ggplot(data = mdata, aes(x=Timestep, y=value, group=variable, color=variable)) +
    geom_line(data = mdata[mdata$variable != "noised",], size=1.2) +
    geom_line(data = mdata[mdata$variable == "noised",], size=.5, linetype='dotdash') +
    scale_color_manual(values=c("KF"='blue',"KS"='red', "sinusoid"='black', "noised"='green4'),
                       labels=c("KF"="Filtered", "KS"="Smoothed", "sinusoid"="Signal", "noised"="Signal w/ Noise")) +
    ggtitle('Kalman Smoothing a Sine Wave w/ Varying Noise')
rsq_kf <- cor(signal, filtered$state)^2
rsq_ks <- cor(signal, smoothed$state)^2
rsq_kf
rsq_ks
```

We can see that the $R^2$ value is higher for the Kalman Smoother than the Kalman Filter under the same parameters, as expected.

## Expectation Maximization

Created by Bridgeford.
``` {r gmm}
# Copyright (c) 2016. All rights reserved. 
#
# An implementation of a Gaussian Mixture Model.
#
# written by Eric Bridgeford
#
# Inputs:
#   x[i, d, d]: d dimensional data points
#   means[c, d]: means for our initial guesses
#   covs[c, d, d]: co variance matrices for our initial guesses
#   pi_vec[c]: weight vector for the amount each guess gaussian
#       contributes to the joint distribution
#
gmm <- function(x, means, covs, pi_vec, tol=1e-3) {
  N = dim(x)[1]
  K = dim(pi_vec)[1]
  r = array(0, dim=c(N, K))
  converge = FALSE
  ll <- likelihood(x, means, covs, pi_vec)
  
  niter <- 1
  while (!isTRUE(converge)) {
    
    # E step, compute the responsiblities
    for (i in 1:N) {
      total <- sum(sapply(1:K, function(c) prob_gauss(x[i,], means[,c], covs[,,c])))
      for (c in 1:K) {
        r[i, c] = pi_vec[c] * prob_gauss(x[i,], means[,c], covs[,,c])/total
      }
    }
    
    # M step, compute the new parameters for our model
    
    m <- array(apply(r, 2, sum))
    
    pi_vec <- m/sum(m)
    
    means <- sapply(1:length(m), function(c) {
      1/m[c] * t(r[,c]) %*% x
    })
    
    covs <- sapply(1:length(m), function(c) {
      1/m[c] * apply(sapply(1:dim(r)[1], function(i)
        (r[i,c] * (x[i,]-means[,c]) %*% t(x[i,]-means[,c])), simplify='array'), c(1,2), sum)
    }, simplify='array')
    
    new_ll <- likelihood(x, means, covs, pi_vec)
    if (isTRUE(abs(new_ll - ll) < tol)) {
      converge <- TRUE
    } else {
      ll = new_ll
      niter <- niter + 1
    }
  }
  
  return(list(means=means, covs=covs, pi_vec=pi_vec, n=niter))
}

likelihood <- function(x, means, covs, pi_vec) {
  ll <- 0
  N <- dim(x)[1]
  K <- dim(pi_vec)[1]
  for (i in 1:N) {
    ll <- ll + sum(sapply(1:K, function(c) pi_vec[c] * prob_gauss(x[i,], means[,c], covs[,,c])))
  }
  return(ll)
}

prob_gauss <- function(point, mean, cov) {
  return(dmvnorm(x=point, mean=mean, sigma=cov))
}
```

An example:

```{r EMex, message=FALSE, warning=FALSE}
  # 3 cluster example
  draw_mus <- array(0, dim=c(2,3))
  draw_covs <- array(0, dim=c(2,2,3))
  draw_mus[,1] <- c(2,3)
  draw_mus[,2] <- c(8,12)
  draw_mus[,3] <- c(6, 4)
  draw_covs[,,1] <- 2*diag(2)
  draw_covs[,,2] <- 3*diag(2)
  draw_covs[,,3] <- 1.5*diag(2)
  colors <- c('blue', 'red', 'green')

  data <- data.frame(x=c(), y=c(), cluster=c())
  x <- array(0, dim=c(100, 2, 3))
  for (i in 1:3) {
    x[,,i] <- mvrnorm(n=100, draw_mus[,i], draw_covs[,,i])
  }
  truth_mus <- array(0, dim=c(2,3))
  truth_covs <- array(0, dim=c(2,2,3))
  for (i in 1:3) {
    truth_mus[,i] <- apply(x[,,i], 2, mean)
    truth_covs[,,i] <- cov(x[,,i])
    data <- rbind(data, data.frame(x[,,i], cluster=colors[i]))
  }

  x <- abind(x[,,1], x[,,2], x[,,3], along=1)

  ggplot(data = data, aes(x = X1, y = X2, color=cluster)) + 
    geom_point() +
    ylab("") +
    xlab("") +
    ggtitle("2d Gaussian, 3 cluster example")

  pi_vec <- array(c(.333, .333, .333))
  test_mus <- array(0, dim=c(2,3))
  test_covs <- array(0, dim=c(2,2,3))
  test_mus[,1] <- c(0,0)
  test_mus[,2] <- c(5,6)
  test_mus[,3] <- c(2,3)
  test_covs <- sapply(1:3, function(x) diag(2), simplify='array')

  fig <- ggplot() + geom_point(data = data, aes(x = X1, y = X2, color=cluster))
  for (i in 1:3) {
    ellipse <- data.frame(ellipse(test_covs[,,i], centre=test_mus[,i]))
    fig <- fig + geom_polygon(data=ellipse, aes(x=x, y=y, fill='black', alpha=0.1, group=i))
  }
  fig <- fig +
    xlab('') +
    ylab('') +
    ggtitle('Sample shown with arbitrary mixture of gaussians') +
    theme(legend.position = 'none')
  fig

  model <- gmm(x, test_mus, test_covs, pi_vec, tol=0.001)
  model$means
  truth_mus
  model$covs
  truth_covs
  fig <- ggplot() + geom_point(data = data, aes(x = X1, y = X2, color=cluster))
  for (i in 1:3) {
    ellipse <- data.frame(ellipse(model$covs[,,i], centre=model$means[,i]))
    fig <- fig + geom_polygon(data=ellipse, aes(x=x, y=y, fill='black', alpha=0.1, group=i))
  }
  fig <- fig +
    xlab('') +
    ylab('') +
    ggtitle('Sample shown with GMM-fitted mixture of gaussians') +
    theme(legend.position = 'none')
  fig
```