---
title: "ewok_week_919"
author: "Eric Walker"
date: "September 19, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Kalman Filter EM algorithm

The Expectation Maximization algorithm for estimating parameters consists of a two-step process whereby a probability distribution over a weighted set of all possible completions of missing data is guessed using the current estimates of the parameters (E-step) and then this is used to create a new estimate of the parameters by maximizing the expected log-likelihood (M-step). These steps are repeated in an alternating fashion until the difference between the new estimates and the previous estimates falls below a specified threshold.

### Univariate

```{r kfsu, include=FALSE}
KFSu <- function(A,C,Q,R,x0,v0,y) {
  
  T=length(y)
  Fv1=vector(,T)
  Fv2=vector(,T)
  Fx1=vector(,T)
  Fx2=vector(,T)
  Sx=vector(,T)
  Sv=vector(,T)
  Scov=vector(,T)
  
  CRC = C * (1/R) * C
  
  # Filter
  for (t in 1:T) {
    if (t==1) {
      Fv1[t]=v0-(v0*CRC*v0 - v0*CRC*1/((1/v0)+CRC)*CRC*v0)
      Fv2[t]=A*Fv1[t]*A+Q
      Fx1[t]=x0+v0*C*(1/R)*(y[t]-C*x0)-v0*CRC*1/((1/v0)+CRC)*C*(1/R)*(y[t]-C*x0)
      Fx2[t]=A*Fx1[t]
    } else {
      v2=Fv2[t-1]
      Fv1[t]=v2-v2*CRC*v2+v2*CRC*1/((1/v2)+CRC)*CRC*v2
      Fv2[t]=A*Fv1[t]*A+Q
      x2=Fx2[t-1]
      Fx1[t]=x2 + v2*C*(1/R)*(y[t]-C*x2)-v2*CRC*1/((1/v2) +CRC)*C*(1/R)*(y[t]-C*x2)
      Fx2[t]=A*Fx1[t]
    }
  }
  
  # Smoother
  for (i in 1:T) {
    t=T-i+1
    if (t==T) {
      Sv[t]=Fv1[t]
      Jt=Fv1[t]*A/(Fv2[t])
      Scov[t]=Fv2[t]*Jt
      Sx[t]=Fx1[t]+Jt*(Fx2[t]-A*Fx1[t])
    } else {
      Jt=Fv1[t]*A/(Fv2[t])
      Sv[t]=Fv1[t]+Jt*(Sv[t+1]-Fv2[t])*Jt
      Scov[t]=Sv[t+1]*Jt
      Sx[t]=Fx1[t]+Jt*(Sx[t+1]-A*Fx1[t])
    }
  }
  return(list(Fv1=Fv1,Fv2=Fv2,Fx1=Fx1,Fx2=Fx2,Sx=Sx,Sv=Sv,Scov=Scov))
}
```

Multivariate Kalman Filter & Smoother:
```{r kfs}
KFS <- function(A,C,Q,R,x0,v0,y) {

  require('MASS')
  T=dim(y)[2]
  d=dim(A)[1]
  Fv1=array(0, dim=c(d,d,T))
  Fv2=array(0, dim=c(d,d,T))
  Fx1=array(0, dim=c(d,T))
  Fx2=array(0, dim=c(d,T))
  Sx=array(0, dim=c(d,T))
  Sv=array(0, dim=c(d,d,T))
  Scov=array(0, dim=c(d,d,T))
  
  CRC = t(C) %*% ginv(R) %*% C
  
  # Filter
  for (t in 1:T) {
    if (t==1) {
      Fv1[,,t]=v0-v0%*%CRC%*%v0+v0%*%CRC%*%ginv(ginv(v0)+CRC)%*%CRC%*%v0
      Fv2[,,t]=A%*%Fv1[,,t]%*%t(A)+Q
      Fx1[,t]=x0 + v0%*%t(C)%*%ginv(R)%*%(y[,t]-C%*%x0)-v0%*%CRC%*%ginv(ginv(v0)+CRC)%*%t(C)%*%ginv(R)%*%(y[,t]-C%*%x0)
      Fx2[,t]=A%*%Fx1[,t]
    } else {
      v2=Fv2[,,t-1]
      Fv1[,,t]=v2-v2%*%CRC%*%v2+v2%*%CRC%*%ginv(ginv(v2)+CRC)%*%CRC%*%v2
      Fv2[,,t]=A%*%Fv1[,,t]%*%t(A)+Q
      x2=Fx2[,t-1]
      Fx1[,t]=x2 + v2%*%t(C)%*%ginv(R)%*%(y[,t]-C%*%x2)-v2%*%CRC%*%ginv(ginv(v2)+CRC)%*%t(C)%*%ginv(R)%*%(y[,t]-C%*%x2)
      Fx2[,t]=A%*%Fx1[,t]
    }
  }
  
  # Smoother
  for (i in 1:T) {
    t=T-i+1
    if (t==T) {
      Sv[,,t]=Fv1[,,t]
      Jt=Fv1[,,t]%*%t(A)/(Fv2[,,t])
      Jt[is.na(Jt)] <- 0
      Scov[,,t]=Fv2[,,t]%*%t(Jt)
      Sx[,t]=Fx1[,t]+Jt%*%(Fx2[,t]-A%*%Fx1[,t])
    } else {
      Jt=Fv1[,,t]%*%t(A)/(Fv2[,,t])
      Jt[is.na(Jt)] <- 0
      Sv[,,t]=Fv1[,,t]+Jt%*%(Sv[,,t+1]-Fv2[,,t])%*%t(Jt)
      Scov[,,t]=Sv[,,t+1]%*%t(Jt)
      Sx[,t]=Fx1[,t]+Jt%*%(Sx[,t+1]-A%*%Fx1[,t])
    }
  }
  return(list(Fv1=Fv1,Fv2=Fv2,Fx1=Fx1,Fx2=Fx2,Sx=Sx,Sv=Sv,Scov=Scov))
}
```

I used the following equation for the likelihood:

$$\frac{T}{2}log|Q^{-1}|-\frac{1}{2}Tr\left(Q^{-1}\left(\sum_{t=0}^{T-1}\left(\textbf{x}_{t+1}-A\textbf{x}_t\right)\left(\textbf{x}_{t+1}-A\textbf{x}_t\right)^T\right)\right)+\frac{T+1}{2}log|R^{-1}|-\frac{1}{2}Tr\left(R^{-1}\left(\sum_{t=0}^T\left(\textbf{y}_t-C\textbf{x}_t\right)^T\left(\textbf{y}_t-C\textbf{x}_t\right)^T\right)\right)$$

```{r kuem, include=FALSE}
# kalman_uni_em.R
# Created by Ewok on 2016-09-11
# Email: ewalke31@jhu.edu
# Copyright (c) 2016. All rights reserved. 
#
# An implementation of a univariate Expectation Maximization Kalman Filter algorithm.
#
# Inputs:
#   y[t]
#     - the observed data
#     - univariate, t timesteps
#   a, c, q, r, pi, v
#     - initial parameter values
#     - state transition, generative, state noise, output noise, state mean, covariance
#   max_i
#     - maximum number of iterations before cut-off
#     - default of 20
#   tol
#     - minimum difference between iterations before cut-off
#     - default of 0.01
#
# Outputs:
#   A, C, Q, R, Pi, V
#     - final parameter estimations
#     - equivalent to respectively named inputs
#   Sx
#     - kalman smoothed values
#   lik
#     - log likelihood
#   
kalman_uni_em <- function(y, a, c, q, r, pi, v, max_i=20, tol=0.01) {

  A <- a
  C <- c
  Q <- q
  R <- r
  Pi <- pi
  V <- v
  n <- length(y)  # number of timesteps
  i <- 1  # iteration
  diff <- 1  # difference between iterations
  Pt <- vector(, n)
  Ptt_1 <- vector(, n-1)
  lik <- vector(, max_i)
  
  while (i <= max_i && diff > tol) {
    
    #  E Step:
    s <- KFSu(A, C, Q, R, Pi, V, y)
    for (t in 1:n) {
      Pt[t] <- s$Sv[t]+s$Sx[t]*s$Sx[t]
      if (t != n) {
        Ptt_1[t] <- s$Scov[t]+s$Sx[t+1]*s$Sx[t]
      }
    }
    like = (n/2)*log(1/Q)-0.5*(1/Q)*sum((s$Fx2-A*s$Fx1)^2)+((n+1)/2)*log(1/R)-0.5*(1/R)*sum((y-C*s$Fx1)^2)
    lik[i] = log(like)
    
    #  M Step:
    C_new <- 1  # constraint
    R_new <- (sum(y*y)-sum(C_new*s$Sx*y)) / n
    A_new <- sum(Ptt_1) / sum(Pt[1:n-1])
    Pi_new <- s$Sx[1]
    Q_new <- 1  # constraint
    V_new <- V  # constraint
    
    # updates
    diff <- abs(C_new-C)+abs(R_new-R)+abs(A_new-A)+abs(Q_new-Q)+abs(Pi_new-Pi)+abs(V_new-V)
    i <- i+1
    A <- A_new
    C <- C_new
    Q <- Q_new
    R <- R_new
    Pi <- Pi_new
    V <- V_new
  }
  lik=lik[lik!=0]
  return(list(A=A, C=C, Q=Q, R=R, Pi=Pi, V=V, Sx=s$Sx, lik=lik))
}
```

Consider the following example, where the initial state mean is started at the correct value:
```{r ex, warning=FALSE}
t <- seq(0,4*pi,,100)
actual <- sin(t) + sin(2*t)
measure <- actual + rnorm(100, mean=0, sd=0.25)
x <- kalman_uni_em(measure, 1, 1, 1, 1, 0, 0, 100, 1e-5)
plot(t, x$Sx, main="EM KF Algorithm on Sine Wave", xlab="Time", ylab="Value")
lines(t, measure, col="blue")
lines(t, actual, col="red")
mse <- sum((actual-x$Sx)^2)
mse
plot(x$lik, main="Log Likelihood of EM KF Algorithm", xlab="Iteration", ylab="Log Likelihood")
```

### Multivariate

Multivariate code:
```{r kem}
# kalman_em.R
# Created by Ewok on 2016-09-17
# Email: ewalke31@jhu.edu
# Copyright (c) 2016. All rights reserved. 
#
# An implementation of a multivariate Expectation Maximization Kalman Filter algorithm.
#
# Inputs:
#   y[t, n]
#     - the observed data
#   a, c, q, r, pi, v
#     - initial parameter values
#     - state transition, generative, state noise, output noise, state mean, covariance
#   max_i
#     - maximum number of iterations before cut-off
#     - default of 20
#   tol
#     - minimum difference between iterations before cut-off
#     - default of 0.01
#
# Outputs:
#   A, C, Q, R, Pi, V
#     - final parameter estimations
#     - equivalent to respectively named inputs
#   Sx
#     - kalman smoothed values
#   lik
#     - log likelihood
#   
kalman_em <- function(y, a, c, q, r, pi, v, max_i=20, tol=0.01) {
  
  source('C:/Users/Eric/Documents/R/KFS.R')
  A <- a
  C <- c
  Q <- q
  R <- r
  Pi <- pi
  V <- v
  y <- t(y)
  n <- dim(y)[2]  # number of timesteps
  d <- dim(A)[1]
  i <- 1  # iteration
  diff <- 1  # difference between iterations
  Pt <- array(0, dim=c(d,d,n))
  Ptt_1 <- array(0, dim=c(d,d,n-1))
  #lik <- array(0, dim=c(d,d,max_i))
  
  while (i <= max_i && diff > tol) {
    
    #  E Step:
    s <- KFS(A, C, Q, R, t(Pi), V, y)
    for (t in 1:n) {
      Pt[,,t] <- s$Sv[,,t]+s$Sx[,t]%*%t(s$Sx[,t])
      if (t != n) {
        Ptt_1[,,t] <- s$Scov[,,t]+s$Sx[,t+1]%*%t(s$Sx[,t])
      }
    }
    #like = (n/2)*log(solve(Q))-0.5*(solve(Q))%*%sum((s$Fx2-A%*%s$Fx1)^2)+((n+1)/2)*log(solve(R))-0.5*(solve(R))%*%sum((y-C%*%s$Fx1)^2)
    #lik[i] <- log(like)
    
    #  M Step:

    C_new <- y%*%t(s$Sx) / apply(Pt, c(1,2), sum)
    C_new <- C_new / norm(C_new, type="2")
    vR_new <- (apply(y*y, 1, sum)-apply((C_new%*%s$Sx)*y, 1, sum)) / n
    #R_new <- diag(vR_new)
    R_new <- vR_new[1]*diag(d)
    #R_new <- mean(vR_new)*diag(d)
    A_new <- apply(Ptt_1, c(1,2), sum) / apply(Pt[,,1:n-1], c(1,2), sum)
    Pi_new <- array(s$Sx[,1], dim=c(1,d))
    Q_new <- diag(d)  # constraint
    V_new <- V  # constraint
    
    # updates
    diff <- norm(C_new-C)+norm(R_new-R)+norm(A_new-A)+norm(Q_new-Q)+norm(Pi_new-Pi)+norm(V_new-V)
    i <- i+1
    A <- A_new
    C <- C_new
    Q <- Q_new
    R <- R_new
    Pi <- Pi_new
    V <- V_new
  }
  #lik=lik[lik!=0]
  return(list(A=A, C=C, Q=Q, R=R, Pi=Pi, V=V, Sx=s$Sx))#, lik=lik))
}
```

```{r ex2, warning=FALSE}
t <- seq(0,4*pi,,100)
actual1 <- sin(t) + sin(2*t)
actual2 <- sin(t) + sin(2*t) + 3
actual = array(actual1, dim=c(100,2))
actual[,2] <- actual2
x <- kalman_em(actual, diag(2), diag(2), diag(2), diag(2), array(c(0,3), dim=c(1,2)), diag(2), 100, 1e-5)
plot(x$Sx[1,], ylim=c(-10,5), xlab="Timestep", ylab="Value", main="MV EM KF Algorithm Sine Example")
lines(x$Sx[2,], type="p", col="blue")
lines(actual1, col="black")
lines(actual2, col="blue")
```

### n4sid

Very basic summary. Need to investigate more in-depth.

Major advantages:

* Non-iterative w/ no non-linear optimization needed, so it does not suffer from local minimas and sensitivity to initial estimates
* No difference between zero and non-zero initial states

Given input and output data, the n4sid algorithm finds the Kalman states, then uses least squares to estimate the system matrices. This is opposed to the conventional way of getting the system matrices and then applying the Kalman filter to get the Kalman states.