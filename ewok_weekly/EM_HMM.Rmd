---
title: "EM_HMM"
author: "Eric Walker & Tanay Agarwal"
date: "October 31, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Pseudocode

### Input

O - matrix of n observations of m dimensions each (n x m)  
X - number of hidden states to use when modeling HMM (const)  
e - stopping epsilon to determine when difference in log-likelihood is "insignificant" (const)  
A.in - initial guess of A (transition probabilities)  
Pi.in - initial guess of Pi (starting probabilities)  
B.in - initial guess of B (emission probabilities)  

### Output

A - $P(x'|x)$  - probability of next latent state given current latent state (d x d)  
Pi - $P(x_1)$  - probability of initial latent state (1 x m)  
B - $P(o|x)$  - probability of observed state given latent state (n x d)  

### Function Name

EM_HMM

### Algorithm Pseudocode

Store inputs O, X, e, A, pi, B  
Calculate current log-likelihood  
while (current log likelihood - previous log likelihood > e) {  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;E step:  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;For each o:  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Compute alpha, beta, and P(o) from forward-backward algorithm.  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\alpha_i(t) = P(o_1,o_2,...,o_t, x_t = i|\mu)$  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\beta_i(t) = P(o_{t+1},o_{t+2},..., o_{T} |x_t = i, \mu)$  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Compute $P(x_1 | o), P(x_t, o_t | o), P(x_t, x_{t+1} | o)$  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Computed expected counts $C(x_1), C(x, o), C(x, x')$ by summing over all o  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;M step: Calculate new theta using equations specified below  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Calculate new log-likelihood  
}  
Return final parameters.  

# Simulation Description

### Good Performance (Simulation 1)

We will create an HMM object using the HMM package in R with random matrices for the parameters. We will then simulate a list of states and observations from this object. We will generate random matrices for the transition matrix, emission matrix, and starting probabilities. We will run EM_HMM with these as parameters and we expect to find that they closely match the transition matrix, emission matrix, and starting probabilities used to generate the HMM object.

### Poor Performance (Simulation 2)

### Evaluating Performance

We will take the mean squared error of A, B, and Pi (estimated vs actual). MSE will be measured by taking the cell-by-cell difference between the actual and estimated parameters, squaring it, summing over all cells, and dividing by total number of cells. We want the MSE of each to be as close to 0 as possible.

### Simulation Code

```{r sim, message=FALSE, warning=FALSE}
require("HMM")

transitionMatrix = matrix(data=NA, nrow=3, ncol=3)  # generate a random transition matrix
for(i in 1:3) {
  vec = runif(3,0,1)
  vec = vec * (1/sum(vec))
  transitionMatrix[i,] = vec
}

emissionMatrix = matrix(data=NA, nrow=3, ncol=3)  # generate a random emission matrix
for(i in 1:3) {
  vec = runif(3,0,1)
  vec = vec * (1/sum(vec))
  emissionMatrix[i,] = vec
}

vec = runif(3,0,1)
startProbs = vec * (1/sum(vec))

# create a new markov model using variables above
# simulate a path of states and observations
model = initHMM(States=c("A","B","C"), Symbols=c("X","Y","Z"), startProbs=startProbs, transProbs=transitionMatrix, emissionProbs=emissionMatrix)
x = simHMM(model, 100)

print(transitionMatrix)
print(emissionMatrix)
print(startProbs)
print(x)

# Now we randomly initialize A, Pi, and B to pass into the EM algorithm
A.in = matrix(data=NA, nrow=3, ncol=3)
for(i in 1:3) {
  a_vec = runif(3,0,1)
  a_vec = a_vec * (1/sum(a_vec))
  A.in[i,] = a_vec
}

pi_vec = runif(3,0,1)
pi_vec = pi_vec * (1/sum(pi_vec))
pi.in = matrix(data=pi_vec, nrow=1, ncol=3)

B.in = matrix(data=NA, nrow=3, ncol=10)
for(i in 1:3) {
  b_vec = runif(10,0,1)
  b_vec = b_vec * (1/sum(b_vec))
  B.in[i,] = b_vec
}
```

After we run EM_HMM, we want A, B, and Pi to match transitionMatrix, emissionMatrix, and startProbs as closely as possible.

# Algorithm Code

# Simulate and Evaluate

### Simulation 1

### Simulation 2

# Final Comments