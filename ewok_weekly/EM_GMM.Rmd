---
title: "Expectation Maximization for Gaussian Mixture Models"
author: "Tanay Agarwal & Eric Walker"
date: "October 26, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Step 1: Pseudocode

## Input

X - matrix (n x m) of n observations of m dimensions each

k - number of Gaussians to use when modeling mixture

e - stopping epsilon to determine when difference in log-likelihood is "insignificant"

A.in - starting alphas

U.in - starting means

C.in - starting variances

## Output

U - set of estimated means

C - set of estimated variances

A - set of estimated alphas

$\theta$ - set {A, U, C}

## Function Name

EM_GMM(X, K, e, A.in, U.in, C.in)

## Algorithm Pseudocode

```{r, eval=FALSE}
Store inputs X, k
Initialize random theta
Calculate log-likelihood of current theta

while (log likelihood of current theta - log likelihood of previous theta > e) {
  E step: Compute member weights over all X and mixture components -> N x K matrix of weights
  M step: Calculate new theta using equations specified below
  Calculate log-likelihood of new theta
}

Return final theta set
```

$a_k$ = $\frac{N_k}{N}$

$u_k$ = $\frac{1}{N_k}$ $\sum_{i=1}^{N}$ $w_{ik}*x_i$

$\sigma_k$ = $\frac{1}{N_k}$ $\sum_{i=1}^{N}$ $w_{ik}*(x_i - u_k)(x_i-u_k)^T$

# Step 2: Simulations

## Success

X matrix (1000 x 1) with data generated by 1 Gaussian distribution (0, 1), k = 1, e = 0.001

X matrix (2000 x 1) with data generated by 2 Gaussian distributions (0, 1) and (5, 1), k = 2, e = 0.001

X matrix (1000 x 2) with data generated by 1 Gaussian distribution ([0, 0], [1, 1]), k = 1, e = 0.001

X matrix (3000 x 3) with data generated by 3 Gaussian distributions ([0, 0, 0], [1, 1, 1]) and ([1, 1, 1], [1, 1, 1]) and ([3, 3, 3], [5, 5, 5]), k = 3, e = 0.001

## Failure

X matrix (2000 x 1) with data generated by 2 Gaussian distribution (0, 1) and (5, 1), k = 1, e = 0.001

X matrix (1000 x 1) with data generated by 1 exponential distribution (2), k = 1, e = 0.001

X matrix (2000 x 1) with data generated by 2 exponential distributions (2) and (5), k = 2, e = 0.001

# Step 3: Choose Visualization

We will use a scatter plot in which the clusters are shown as the color of the points belonging to them, with a legend identifying cluster ID.

# Step 4: Evaluating Performance

We will qualitatively check to see if the predicted parameters fit/match the actual plots. We will quantitatively assess the estimated theta by checking the numerical difference between the estimated and actual parameters.

# Step 5: Simulation Code

```{r, message=FALSE, warning=FALSE}
library("ggplot2")
library("dplyr")
library("reshape2")

options(scipen = 999)
set.seed(1)

comp1.vals <- data_frame(comp = "A", 
                         vals = rnorm(200, mean = 0, sd = 1))
comp2.vals <- data_frame(comp = "B", 
                         vals = rnorm(200, mean = 2, sd = 1))
comp3.vals <- data_frame(comp = "C",
                         vals = rexp(200, rate = 1))

#comp1.vals <- rnorm(200, mean = 0, sd = 1)
```

# Step 6: Simulation Data Plots

```{r, message=FALSE}
vals.df <- bind_rows(comp1.vals, comp2.vals)

vals.df %>%
  ggplot(aes(x = vals, y = "A", color = factor(comp))) +
  geom_point(alpha = 0.4) +
  scale_color_discrete(name = "Source of Data") +
  xlab("Values") +
  theme(axis.ticks.y = element_blank(),
        axis.text.y = element_blank(),
        axis.title.y = element_blank(),
        legend.position = "top")

vals.df <- bind_rows(comp1.vals)

vals.df %>%
  ggplot(aes(x = vals, y = 0)) +
  geom_point(alpha = 0.4) +
  xlab("Values") +
  theme(axis.ticks.y = element_blank(),
        axis.text.y = element_blank(),
        axis.title.y = element_blank())

vals.df <- bind_rows(comp3.vals)

vals.df %>%
  ggplot(aes(x = vals, y = 0)) +
  geom_point(alpha = 0.4) +
  xlab("Values") +
  theme(axis.ticks.y = element_blank(),
        axis.text.y = element_blank(),
        axis.title.y = element_blank())
```

# Step 7: Algorithm Code

```{r}
require('mvtnorm')
EM_GMM <- function(X, K, e, A.in, U.in, C.in) {
  N <- length(X)
  
  # initialize theta
  A <- A.in
  U <- U.in
  C <- C.in
  
  logls <- c(0)
  
  repeat {
    # E-step
    w <- matrix(rep(0, len = N*K), nrow = N) # initialize w
    for (i in 1:N) {
      for (k in 1:K) {
        numerator <- (A[k] * dmvnorm(X[i], mean = U[k], sigma = matrix(C[k])))
        denominator <- sum(sapply(1:K, function(m) A[m] * dmvnorm(X[i], mean = U[m], sigma = matrix(C[m]))))
        w[i, k] <- (numerator / denominator)
      }
    }
    
    # M-step
    A <- rep(0, length.out = K)
    for (k in 1:K) { # update alphas
      Nk <- 0
      for (i in 1:N) { # calculate N_k
        Nk <- (Nk + w[i, k])
      }
      A[k] <- (Nk / N)
    }
    
    U <- rep(0, length.out = K)
    for (k in 1:K) { # update means
      Nk <- 0
      temp <- 0
      for (i in 1:N) { # calculate N_k
        Nk <- (Nk + w[i, k])
        temp <- (temp + (w[i, k] * X[i]))
      }
      U[k] <- (temp / Nk)
    }
    
    C <- rep(0, length.out = K)
    for (k in 1:K) { # update variances
      temp <- 0
      Nk <- 0
      for (i in 1:N) { # calculate N_k
        Nk <- (Nk + w[i, k])
        temp <- (temp + (w[i, k] * (X[i] - U[k]) %*% (X[i] - U[k])))
      }
      C[k] <- (temp / Nk)
    }
    
    # calculate log-likelihood
    logl <- 0
    for (i in 1:N) {
      likel <- sum(sapply(1:K, function(k) A[k] * dmvnorm(X[i], mean = U[k], sigma = matrix(C[k]))))
      logl_curr <- log(likel)
      logl <- (logl + logl_curr)
    }
    
    logls <- c(logls, logl)
    
    if (abs(logls[length(logls)] - logls[length(logls) - 1]) < e) {
      break
    }
  }
  
  print(A)
  print(U)
  print(C)
  
  plot(logls, type="l", main="Log Likelihood of GMM", xlab="Iteration", ylab="Log Likelihood", xaxt="n")
  axis(1, at=1:length(logls))
  
}
```

# Step 8: Algorithm Simulation

```{r sim}
sim1 = rnorm(1000, mean=0, sd=1)
sim2 = rnorm(1000, mean=5, sd=1)
sim3 = rnorm(1000, mean=10, sd=2)
sim4 = rexp(1000, rate=2)
sim5 = rexp(1000, rate=5)

print("test 1")
sim = c(sim1)
A = c(1)
U = c(10)
C = c(5)
EM_GMM(sim, 1, 0.001, A, U, C)

print("test 2")
sim = c(sim1, sim2)
A = c(0.2, 0.8)
U = c(10, 16)
C = c(5, 6)
EM_GMM(sim, 2, 0.001, A, U, C)
    
print("test 3")
EM_GMM(sim, 1, 0.001, A, U, C)
    
print("test 4")
sim = c(sim4)
A = c(1)
U = c(10)
C = c(5)  
EM_GMM(sim, 1, 0.001, A, U, C)
    
print("test 5")
sim = c(sim4, sim5)
A = c(0.5, 0.5)
U = c(10, 12)
C = c(5, 16)
EM_GMM(sim, 2, 0.001, A, U, C)

```

# Step 9: Evaluation

We can see from our test results that the algorithm ran as expected on all the simulations for the Gaussian distributions. It returned the proper means for simulations 1 and 2, but erred in simulation 3 when it was given the incorrect number of mixtures as an argument. As expected, it performed poorly on both simulations 4 and 5 that used exponential distributions.
